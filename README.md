

#### ğŸ‘·â€â™€ï¸âš ï¸ Work in Progress âš ï¸ ğŸ‘·â€â™€ï¸

## Dokumentation quenfo_py
***
Die Software **quenfo_py** bietet verschiedene Funktionen zur Verarbeitung von Stellenanzeigen an.
Diese unterteilen sich in die Klassifikation von Stellenanzeigen, in die Informationsextraktion von Kompetenzen und Tools und in Matching-Workflows zum Auffinden bereits bekannter EntitÃ¤ten innerhalb klassifizierter Paragrafen.
In dieser Dokumentation werden die jeweiligen Workflows beschrieben. Dabei werden die einzelnen Schritte und die genutzten Klassen und Methoden aufgefÃ¼hrt. 
Jede ausfÃ¼hrbare Applikation arbeitet mit Object Relational Mapping (ORM). Objekte werden hierbei als DatenbankeintrÃ¤ge persistiert, d.h. in den Datenklassen (z.B. in den Klassen ClassifyUnits, ExtractedEntity, InformationEntity oder ExtractionUnit) werden entsprechende Annotationen an Klassenattributen vorgenommen, um diese als vorzunehmenden Eintrag zu kennzeichnen. FÃ¼r die Realisierung wurde [SQLAlchemy](https://docs.sqlalchemy.org/en/14/orm/) genutzt. 

Die Software entstand im Projekt [Quenfo](https://dh.phil-fak.uni-koeln.de/forschung/qualifikationsentwicklungsforschung) 
und in Kooperation mit dem Bundesinstitut fÃ¼r Berufsbildung.


**Zielsetzung:**

	a. Stellenanzeigen werden in Paragraphen aufgesplittet und klassifiziert.
			MÃ¶gliche Klassen: 
			1. Selbstvorstellung des ausschreibenden Unternehmens
			2. Beschreibung der TÃ¤tigkeit, Angebote an die Bewerberinnen und Bewerber
			3. Anforderungen an die Bewerberin bzw. den Bewerber 
			4. Formalia und Sonstiges
			5. 1&3
			6. 2&3
   
	b. Informationsextraktion von Kompetenzen und Tools aus klassifizierten Paragraphen
	c. Matching-Workflows zum Auffinden bereits bekannter EntitÃ¤ten innerhalb klassifizierter Paragrafen.

**Input:**

	Trainingsdaten (SQL-Datenbank mit bereits klassifizierten Stellenanzeigen)
	Input-Daten (SQL-Datenbank mit Stellenanzeigen)

**Hauptstruktur:**

	Textclassification
	Information Extraction
	Matching

**Output:** SQL-Datenbank bestehend aus:

	SQL-Tabelle mit klassifizierten Paragraphen
	SQL-Tabelle mit ExtractionUnits (Tools oder Kompetenzen)
	SQL-Tabelle mit MatchingUnits (Tools oder Kompetenzen)
	
--> Mehr zu Input und Output siehe Ende der Readme

***
### QuickstartğŸƒ
***
Die Anwendung wurde in Python 3.7 geschrieben.

Klone das Repository

`git clone https://github.com/agerlac1/quenfo_py.git`

cd in den Ordner **quenfo_py/code**: hier liegt die requirements Datei und das Programm wird von hier ausgefÃ¼hrt (working dir)

`python -m pip install -r requirements.txt`

Mit der nachfolgenden AusfÃ¼hrung wird das gesamte Programm samt Default-Settings aufgerufen (Input-und Trainingsdaten mÃ¼ssen zuvor in der quenfo_data
Struktur liegen und die Dateinamen in der config.yaml Datei angegeben werden.)

--> Textclassification, Information Extraction, Matching

`python main.py --input_path "absolute_path_to_input_data" --db_mode {overwrite,append}`

--> Input- und Trainingsdaten mÃ¼ssen in der quenfo_data Struktur liegen.

Informationen Ã¼ber die erfolgten AblÃ¤ufe und Ergebnisse werden in dem Modul `/logger` in den entsprechenden logging-Dateien gespeichert.

***
### WorkflowğŸ”
***
Im Folgenden wird der Workflow der **quenfo_py** Software beschrieben.

#### Allgemein

Allgmein besteht die Software aus 3 bzw. 4 Hauptmodulen. ZunÃ¤chst wird das Trainings-Modul aufgerufen, in welchem das Model geladen oder je nach dem neu trainiert wird.
Danach beginnt der Hauptprozess der Software, in dem die zu verarbeitenden Stellenanzeigen erst klassifiziert werden und dann Informationen zu Kompetenzen oder Tools extrahiert und bereits bekannte EntitÃ¤ten gematched werden.

<img src="docs/quenfo_py.svg"/>

Im folgenden sieht man die Klassenstrukturen der ORM-Models:

<img src="docs/models.jpg"/>


#### Code Struktur
Der Code ist so struktuiert, dass sich die einzelnen Module (im Workflow s.o. erkennbar) ebenfalls in der Ordnerstruktur wiederfinden.
```
ğŸ“¦quenfo_py
 â”£ ğŸ“‚additional_scripts
 â”£ ğŸ“‚code
 â”ƒ â”£ ğŸ“‚classification
 â”ƒ â”ƒ â”£ ğŸ“‚predict_classes
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œknn_predictor.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œregex_predictor.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresult_merger.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_classifyunits
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚classify_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_classifyunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚feature_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_featureunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚feature_vectors
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_featurevectors.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚configuration
 â”ƒ â”ƒ â”£ ğŸ“œconfig_model.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚database
 â”ƒ â”ƒ â”£ ğŸ“œconnection.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚information_extraction
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_extractionunits
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚extraction_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_extractionunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_resources
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconnection_resources.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_entities.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“œhelper.py
 â”ƒ â”ƒ â”£ ğŸ“œmodels.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚logger
 â”ƒ â”£ ğŸ“‚orm_handling
 â”ƒ â”ƒ â”£ ğŸ“œmodels.py
 â”ƒ â”ƒ â”£ ğŸ“œorm.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚tests
 â”ƒ â”£ ğŸ“‚training
 â”ƒ â”ƒ â”£ ğŸ“‚knnclassifier
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_knn.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚regexclassifier
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_regex.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚tfidfvectorizer
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_vectorizer.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“œhelper.py
 â”ƒ â”ƒ â”£ ğŸ“œtrain_models.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“œmain.py
 â”ƒ â”— ğŸ“œrequirements.txt
 â”£ ğŸ“‚docs
 â”£ ğŸ“œ.gitignore
 â”— ğŸ“œREADME.md
```

***
### Implementierung und Module ğŸ› ï¸
***

#### Hauptbestandteile: Training, Classification, IE und Matching
##### Training
Das Trainingsmodul besteht erstens aus dem Abgleich, ob ein Modell bereits vorhanden ist oder ob ein neues trainiert werden soll.
DafÃ¼r wird einmal Ã¼berprÃ¼ft, ob bereits ein Model fÃ¼r den TfidfVectorizer und den KNN-Classifier vorliegt (Dateinamen dazu in config.yaml gesetzt). Liegt jeweils eins vor, wird Ã¼berprÃ¼ft,

1. ob die vorliegenden Trainingsdaten auch fÃ¼r das Modell verwendet wurden und 
2. ob die Konfigurationseinstellungen der Modelle mit denen in der config.yaml gesetzten Ã¼bereinstimmen.
3. ob die Modelle auch gefittet sind.

Sollten dementsprechend neue Trainingsdaten vorliegen oder neue Konfigurationseinstellungen gesetzt worden sein oder die geladenen Modelle nicht gefittet sein, wird neu trainiert oder andersrum der entsprechende Vectorizer und Classifier geladen.
AnschlieÃŸend werden diese fÃ¼r das Objekt der Klasse Model als Werte gesetzt.

Zuletzt wird noch der RegexClassifier geladen, der sich aus den gegebenen Mustern und ihren Klasseneinteilungen aus der Support-Datei *regex.txt*  ergibt. Auch diese werden in Form eines Dataframes als RegexClassifier im Model gesetzt.

<img src="docs/class_model.jpg"/>

Das bedeutet, dass am Ende des Trainingsmoduls ein Objekt der Klasse Model zurÃ¼ckgegeben wird, welches aus den drei Komponenten (Tfidf, KNN & Regex) besteht und noch zusÃ¤tzlich Informationen Ã¼ber die genutzten Trainingsdaten enthÃ¤lt.

##### Classification
Die Textclassification ist in zwei Hauptschritte aufgeteilt:

1. **Vorbereitung der zu klassifizierenden Stellenanzeigen** (*prepare_classifyunits/*) in den Schritten:

	1. Generierung von **classify_units** durch splitten der Stellenanzeigen in Paragraphen (und erste Normalisierungsschritte)
	2. Verarbeitung der Paragraphen zu **feature_units** (Tokenization, Normalization, Stopwords Removal, Stemming, NGram(or ContinuousNGram) Generation)
	3. Vektorisierung der feature_units zu **feature_vectors** mittels des **Tfidf-Vectorizers**(aus dem Objekt Model).

2. **Vorhersage der Klassen fÃ¼r die vorverarbeiteten Paragraphen** (*predict_classes/*) in den Schritten:
	1.  **KNN-Prediction** mittels des KNN-Classifiers aus dem Model.
	2.  **Regex-Prediction** mittels des Regex-Classifiers aus dem Model.
	3. Abgleich und ZusammenfÃ¼hren der beiden Vorhersagen (**merge_results**)


##### Information Extraction
TODO
##### Matching
TODO

#### Support Module

##### configuration
Das configuration-Modul enthÃ¤lt :
1. das **config_models.py** Script, in dem die Klasse *Configuration* definiert wird, die getter, setter und checks fÃ¼r die in der Konfigurationsdatei enthaltenen Werte enthÃ¤lt.
--> Die dazugehÃ¶rige config.yaml Datei (in der die Konfigurationseinstellungen und Filenamen enthalten sind), befindet sich in der quenfo_data Struktur.

##### orm_handling
Das Modul *orm_handling/* ist das VerbindungsstÃ¼ck zwischen Datenbank und Python-Tool. Hier werden Daten abgefragt und in Datenbanken geschrieben, mithilfe der Definition von Models, die die Datenbank-Tabellen abbilden. Verwendet wurde das Python-Package [SQLAlchemy](https://docs.sqlalchemy.org/en/14/orm/) um Object Relational Mapping umzusetzen.

1. `models.py`
Script um Klassen und Schmeata fÃ¼r ORM-Objects zu definieren.
Classes:
        JobAds               --> JobAds to be splitted and classified
        ClassifyUnits        --> preprocessed and classified paragraphs
        TrainData            --> Traindata (already in paragraphs and classified)
        ClassifyUnits_Train  --> contains each Traindata paragraph (preprocessed and classified)
        ExtrationUnits       --> preprocessed and splitted sentences from paragraphs
        InformationEntity    --> extracted entities
2. `orm.py` --> Script enthÃ¤lt query-Abfragen an die Datenbank und Funktionen, um Objekte Datenbanken hinzuzufÃ¼gen (session.add(), session.commit()). AuÃŸerdem werden hier Hilfsfunktionen definiert, die ggf. Tabellen lÃ¶schen und createn. 


##### database
`connection.py`: Script mit dem die connections zu den SQL-Datenbanken hergestellt  werden. --> Returns session-obj und engine-obj.

##### logger
Logging-Ordner, in dem zusÃ¤tzliche Informationen wÃ¤hrend der AusfÃ¼hrung des Tools gespeichert werden.
Es gibt vier verschiedene Logging-files:
1. `log_main.log` --> for all main related processes and raises.
2. `log_clf.log`  --> for all classification related processes and raises.
3. `log_ie.log`   --> for all information extraction related processes and raises.
4. `log_match.log`--> for all matching related processes and raises.

##### tests
TODO

#### Files
**main.py**
Main-Skript des Tools. Hier befindet sich die grobe Architektur und Verwaltung des Programms. Des Weiteren sind hier die ArgumentParser Befehle deklariert, mit denen bestimmte Teile des Skriptes aufgerufen werden kÃ¶nnen (mehr dazu weiter unten).

**requirements.txt**
EnthÃ¤lt eine Auflistung an Python-Dependencies, die benÃ¶tigt werden, um das Tool auszufÃ¼hren.

**input, output**
Input-Path wird Ã¼ber die CMDLine mitgegeben und Output wird in die Input Datenbank reingeschrieben.

***
### ConfigurationğŸ“‹âœ”ï¸
***
In der Datei `config.yaml` sind alle Pfade und einstellbare Parameter vermerkt. Dadurch wird gewÃ¤hrleistet, dass im Code selbst fÃ¼r eine Anwendung nichts verÃ¤ndert werden muss. Alle Ã„nderungen werden in der `config.yaml` Datei vorgenommen.

Ansonsten kÃ¶nnen folgende Werte angepasst werden:
- FeatureUnitConfiguration --> Wie sollen die FeatureUnits erstellt werden?
- Data-Handling Parameter --> Wie viele Stellenanzeigen sollen verarbeitet werden und in welcher Chunksize?
- Tfidf Configuration --> Wie soll der Vectorizer trainiert werden oder welcher soll geladen werden?
- KNN Configuration --> Wie soll der KNN Classifier trainiert werden oder welcher soll geladen werden?
- IE Configuration --> Wie soll die Information Extraction ablaufen?
- Model Paths --> Pfade zu den Modellen (Tfidf und KNN)
- Paths --> Resource Pfade zu den BenÃ¶tigten Dateien

***
### CommandLine - BefehleğŸ“¢
***
Alle Befehle werden relativ zum Ordner `code/` ausgefÃ¼hrt.

**GrundsÃ¤tzlich:** 

    usage: main.py [-h] [--classification] [--extraction] [--matching]
               [--input_path INPUT_PATH] [--db_mode {overwrite,append}]

    classify jobads and extract/match information

    optional arguments:
    -h, --help            show this help message and exit
    --classification
    --extraction
    --matching
    --input_path INPUT_PATH
    --db_mode {overwrite,append}

**Beispiel**

`python main.py --classification --input_path "this/is/my/input/path.db --db_mode overwrite`
--> Hier wird nur die Classification aufgerufen und die im input_path mitgegebene Datei verarbeitet. Sollten bereits ClassifyUnits vorhanden sein, werden diese Ã¼berschrieben.

`python main.py --input_path "this/is/my/input/path.db --db_mode append`
--> Da hier kein Wert mitgegeben wurde, welcher Teil des Tools aufgerufen werden soll, werden alle drei Steps nacheinander durchlaufen (1. Classification, 2. IE, 3. Matching). Da der db_mode *append*  gesetzt wurde, werden ClassifyUnits (die bereits gegeben sein kÃ¶nnten in der input_db) nicht Ã¼berschrieben und nur noch nicht verarbeitete hinzugefÃ¼gt.

`python main.py --classification --extraction --input_path "this/is/my/input/path.db --db_mode overwrite`
--> Hier wird erst die Classification und dann die IE aufgerufen und die im input_path gegebenen Daten verarbeitet. Der db_mode ist auf *overwrite* gesetzt. Dementsprechend werden, falls ClassifyUnits bereits vorhanden sind, diese Ã¼berschrieben.


***
### Daten - AufbauğŸ“š
***

#### Input
--> Datei muss in der entsprechenden quenfo_data Struktur liegen.
Als Input-Dateien mÃ¼ssen SQL-Datenbanken vorliegen. Die Tabelle mit den enthaltenen Stellenanzeigen sollte bestenfalls den Namen *jobads*  haben oder der neue Tabellenname muss manuell im Script *code/orm_handling/models.py *geÃ¤ndert werden. Die Daten mÃ¼ssen mindestens Ã¼ber folgende gefÃ¼llte Spalten verfÃ¼gen, damit sie als Input-Daten verwendet werden kÃ¶nnen (egal ob als Test- oder Trainingsdaten):

- id
- content (Text der Stellenanzeige)
- postingID
- language
- jahrgang

#### Output

Tabelle zur Textclassification:
-  id
- classID
- parentID --> Zu welcher JobAd der Paragraph gehÃ¶rt
- paragraph

Tabelle zur Information Extraction:
Kompetenzen oder Tools werden als EntitÃ¤ten durch Extraktionsmuster extrahiert
- id
- positionIndex
- paragraph_id
- sentence
- tokenArray

Tabelle zum Matching:
Kompetenzen oder Tools werden durch StringMatching gefunden
- id
- parent_id
- type
- startLemma
- singleWordEntitiy
- lemmaArray
- lemmaExpression
- modifier

#### Trainingsdaten als Input
--> Angabe des Dateinamens in config.yaml notwendig --> Datei muss in der spezifischen quenfo_data Struktur liegen.
--> Trainingsdaten als SQLite Datenbank 
--> Tablename = 'traindata' (oder AbÃ¤nderung im Code orm.py)
Folgende Spalten mÃ¼ssen vorliegen:
- content
- classID
- index
- postingID
- zeilennr

--> ZusÃ¤tzliches file *make_sql_traindata.py*  in *additional_scripts/* Folder kann benutzt werden um tsv-Dateien in SQLite Datenbank zu konvertieren.



