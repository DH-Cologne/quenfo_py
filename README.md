

#### ğŸ‘·â€â™€ï¸âš ï¸ Work in Progress âš ï¸ ğŸ‘·â€â™€ï¸


## Dokumentation quenfo_py
***
Die Software **quenfo_py** bietet verschiedene Funktionen zur Verarbeitung von Stellenanzeigen an.
Diese unterteilen sich in die Klassifikation von Stellenanzeigen, in die Informationsextraktion von Kompetenzen und Tools und in Matching-Workflows zum Auffinden bereits bekannter EntitÃ¤ten innerhalb klassifizierter Paragrafen.
In dieser Dokumentation werden die jeweiligen Workflows beschrieben. Dabei werden die einzelnen Schritte und die genutzten Klassen und Methoden aufgefÃ¼hrt. 
Jede ausfÃ¼hrbare Applikation arbeitet mit Object Relational Mapping (ORM). Objekte werden hierbei als DatenbankeintrÃ¤ge persistiert, d.h. in den Datenklassen (z.B. in den Klassen ClassifyUnits, ExtractedEntity, InformationEntity oder ExtractionUnit) werden entsprechende Annotationen an Klassenattributen vorgenommen, um diese als vorzunehmenden Eintrag zu kennzeichnen. FÃ¼r die Realisierung wurde [SQLAlchemy](https://docs.sqlalchemy.org/en/14/orm/) genutzt. 

Die Software entstand im Projekt [Quenfo](https://dh.phil-fak.uni-koeln.de/forschung/qualifikationsentwicklungsforschung) 
und in Kooperation mit dem Bundesinstitut fÃ¼r Berufsbildung.


**Zielsetzung:**

	a. Stellenanzeigen werden in Paragraphen aufgesplittet und klassifiziert.
MÃ¶gliche Klassen: 
1. Selbstvorstellung des ausschreibenden Unternehmens
2. Beschreibung der TÃ¤tigkeit, Angebote an die Bewerberinnen und Bewerber
3. Anforderungen an die Bewerberin bzw. den Bewerber 
4. Formalia und Sonstiges
5. 1&3
6. 2&3

   
	b. Informationsextraktion von Kompetenzen und Tools aus klassifizierten Paragraphen
	c. Matching-Workflows zum Auffinden bereits bekannter EntitÃ¤ten innerhalb klassifizierter Paragrafen.

**Input:**

	1. Trainingsdaten (SQL-Datenbank mit bereits klassifizierten Stellenanzeigen)
	2. Input-Daten (SQL-Datenbank mit Stellenanzeigen)

**Hauptstruktur:**

	1. Classification
	2. Information Extraction
	3. Matching

**Output:** SQL-Datenbank bestehend aus:

	1.  SQL-Tabelle mit klassifizierten Paragraphen
	2.
	3.

***
### QuickstartğŸƒ
***
Die Anwendung wurde in Python 3.7 geschrieben.

Klone das Repository

`git clone https://github.com/agerlac1/quenfo_py.git`

cd in den Ordner **quenfo_py/code**: hier liegt die requirements Datei und das Programm wird von hier ausgefÃ¼hrt (working dir)

`python -m pip install -r requirements.txt`

Mit der nachfolgenden AusfÃ¼hrung wird das gesamte Programm samt Default-Settings aufgerufen (Pfade zu Testdaten und Trainingsdaten mÃ¼ssen zuvor in der config.yaml angegeben werden).

`python main.py --input_path "path_to_input_data" --db_mode {overwrite,append}`

Informationen Ã¼ber die erfolgten AblÃ¤ufe und Ergebnisse werden in dem Modul `/logger` in den entsprechenden logging-Dateien gespeichert.

***
### WorkflowğŸ”
***
Hier kommt der Workflow hin

#### Allgemein

Hier kommt der allgemeine Workflow hin

#### Aufteilung Classification, IE und Matching

Das sind die drei Steps

und hier bitte das Workflow bild einbinden

<img src=""/>


#### Code Struktur
Der Code ist so struktuiert, dass sich die einzelnen Module (im Workflow s.o. erkennbar) ebenfalls in der Ordnerstruktur wiederfinden.
```
ğŸ“¦quenfo_py
 â”£ ğŸ“‚additional_scripts
 â”£ ğŸ“‚code
 â”ƒ â”£ ğŸ“‚classification
 â”ƒ â”ƒ â”£ ğŸ“‚predict_classes
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œknn_predictor.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œregex_predictor.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œresult_merger.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_classifyunits
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚classify_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_classifyunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚feature_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_featureunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚feature_vectors
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_featurevectors.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚configuration
 â”ƒ â”ƒ â”£ ğŸ“œconfig.yaml
 â”ƒ â”ƒ â”£ ğŸ“œconfig_model.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚database
 â”ƒ â”ƒ â”£ ğŸ“œconnection.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚information_extraction
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_extractionunits
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚extraction_units
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_extractionunits.py
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚prepare_resources
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconnection_resources.py
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œconvert_entities.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“œhelper.py
 â”ƒ â”ƒ â”£ ğŸ“œmodels.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚logger
 â”ƒ â”£ ğŸ“‚orm_handling
 â”ƒ â”ƒ â”£ ğŸ“œmodels.py
 â”ƒ â”ƒ â”£ ğŸ“œorm.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“‚tests
 â”ƒ â”£ ğŸ“‚training
 â”ƒ â”ƒ â”£ ğŸ“‚knnclassifier
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_knn.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚regexclassifier
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_regex.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“‚tfidfvectorizer
 â”ƒ â”ƒ â”ƒ â”£ ğŸ“œgen_vectorizer.py
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”ƒ â”£ ğŸ“œhelper.py
 â”ƒ â”ƒ â”£ ğŸ“œtrain_models.py
 â”ƒ â”ƒ â”— ğŸ“œ__init__.py
 â”ƒ â”£ ğŸ“œmain.py
 â”ƒ â”— ğŸ“œrequirements.txt
 â”£ ğŸ“‚docs
 â”£ ğŸ“œ.gitignore
 â”— ğŸ“œREADME.md
```

***
### Implementierung und Module ğŸ› ï¸
***
#### orm_handling

#### database
1. `connection.py`: Script mit dem die connections zu den SQL-Datenbanken hergestellt werden (Input, Output und Backup-Dateien).

#### tests

#### classification




**main.py**

Main-Skript des Tools. Hier befindet sich die grobe Architektur und Verwaltung des Programms. Des Weiteren sind hier die ArgumentParser Befehle deklariert, mit denen bestimmte Teile des Skriptes aufgerufen werden kÃ¶nnen (mehr dazu weiter unten).

**requirements**

EnthÃ¤lt eine Auflistung an Python-Dependencies, die benÃ¶tigt werden, um das Tool auszufÃ¼hren.

**logger**

Logging-File, in dem zusÃ¤tzliche Informationen wÃ¤hrend der AusfÃ¼hrung des Tools gespeichert werden. AuÃŸerdem befinden sich hier die Evaluation-Reports und Sanity_checks.

**input/, output/, temp/**

Ordner, in denen die Input, Output und Temp-Dateien liegen. Wenn andere Pfade fÃ¼r die Dateien verwendet werden sollen, mÃ¼ssen diese in der config.yaml Datei angepasst werden.

***
### ConfigurationğŸ“‹âœ”ï¸
***
In der Datei config.yaml sind alle Pfade, einstellbare Parameter und der Metadaten-Filter vermerkt. Dadurch wird gewÃ¤hrleistet, dass im Code selbst fÃ¼r eine Anwendung nichts verÃ¤ndert werden muss. Alle Ã„nderungen werden in der `config.yaml` Datei vorgenommen.

**Im aktuellen Zustand befindet sich das Programm in der "Werkseinstellung" und es kÃ¶nnen nach Belieben Modelle trainiert und Daten analysiert werden (mit entsprechenden Pfadangaben zu Test- und Trainingsdaten).**

Ansonsten kÃ¶nnen folgende Werte angepasst werden:

1. Andere **Input-Dateien** auswÃ¤hlen:

	- `train_data` und `test_data`

2. Andere **Output-Datei** auswÃ¤hlen:

	- `output_path`

3. Andere **Temp-Dateien** und Pfade festlegen fÃ¼r Dateien, die schon unique_ids bekommen haben:

	- `id_train_data` und `id_test_data`

4. AuswÃ¤hlen, ob fÃ¼r den Prozess ein **trained** oder **retrained Doc2Vec Modell** verwendet wird (nur relevant, wenn Doc2Vec verwendet wird):

	- `d2v_model_type` -> `type` 

	entweder 'd2v_model' fÃ¼r trained Modell oder 'd2v_remodel' fÃ¼r retrained Modell.


***
### CommandLine - BefehleğŸ“¢
***
Alle Befehle werden relativ zum Ordner `code/` ausgefÃ¼hrt.

**GrundsÃ¤tzlich:** 

    usage: main.py [-h] [--classification] [--extraction] [--matching]
               [--input_path INPUT_PATH] [--db_mode {overwrite,append}]

    classify jobads and extract/match information

    optional arguments:
    -h, --help            show this help message and exit
    --classification
    --extraction
    --matching
    --input_path INPUT_PATH
    --db_mode {overwrite,append}



***
### Input DatenğŸ“š
***
Als Input-Dateien mÃ¼ssen SQL-Datenbanken vorliegen. Die Benennung der darin verzeichneten Tabellen ist irrelevant. Die Daten mÃ¼ssen mindestens Ã¼ber folgende Metadaten verfÃ¼gen, damit sie als Input-Daten verwendet werden kÃ¶nnen (egal ob als Test- oder Trainingsdaten):

- full_text

- location_name

- date

- profession_isco_code

- advertiser_name
